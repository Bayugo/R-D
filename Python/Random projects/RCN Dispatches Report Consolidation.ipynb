{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8e98b2b8-be50-402a-ae3d-2c651c86e8d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Import Libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from tabulate import tabulate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "032e49cf-cd1a-4d14-90f7-579ac38ab9dd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['SUMMARY', '2025-QUALITY REP', 'FUNDING', 'SAMPA-PURCHASES', 'SAMPA-DRYING', 'SAMPA-DISPATCHES ', 'SAMPA W.H', 'NKRANKWANTA-PURCHASES', 'NKRANKWANTA-DRYING', 'NKRANKWANTA-DISPATCHES', 'NKRANKWANTA W.H', 'DROBO-PURCHASES', 'DROBO-DRYING', 'DROBO-DISPATCHES', 'DROBO W.H', 'WENCHI-PURCHASES', 'WENCHI-DRYING', 'WENCHI-DISPATCHES', 'WENCHI W.H', 'TECHIMAN-PURCHASES', 'TECHIMAN-DRYING', 'TECHIMAN-DISPATCHES', 'TECHIMAN W.H', 'SAWLA-PURCHASES', 'SAWLA-DRYING', 'SAWLA-DISPATCHES', 'SAWLA W.H ', 'LUC DISP', 'EX-TEMA', 'TOTALS', 'BL & CTNR-WISE REPORTCASHEW', 'Comparisons']\n"
     ]
    }
   ],
   "source": [
    "xls_dispatches = pd.ExcelFile('GHANA ORIGIN REP.xlsx')\n",
    "print(xls_dispatches.sheet_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c675c34e-15dc-4561-aa13-d00062cf528a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Selected dispatches Sheets:\n",
      "SAMPA-DISPATCHES\n",
      "NKRANKWANTA-DISPATCHES\n",
      "DROBO-DISPATCHES\n",
      "WENCHI-DISPATCHES\n",
      "TECHIMAN-DISPATCHES\n",
      "SAWLA-DISPATCHES\n"
     ]
    }
   ],
   "source": [
    "# Corrected list of sheets based on the actual sheet names\n",
    "dispatches_sheets = [\n",
    "    'SAMPA-DISPATCHES ',\n",
    "    'NKRANKWANTA-DISPATCHES',\n",
    "    'DROBO-DISPATCHES',\n",
    "    'WENCHI-DISPATCHES',\n",
    "    'TECHIMAN-DISPATCHES',\n",
    "    'SAWLA-DISPATCHES'\n",
    "]\n",
    "\n",
    "# Selecting the sheets\n",
    "selected_sheets = {}\n",
    "for sheet in dispatches_sheets:\n",
    "    try:\n",
    "        selected_sheets[sheet.strip()] = pd.read_excel(xls_dispatches, sheet_name=sheet)\n",
    "    except ValueError as e:\n",
    "        print(f\"Error loading sheet '{sheet}': {e}\")\n",
    "\n",
    "# Display the names of the successfully loaded sheets\n",
    "print(\"Selected dispatches Sheets:\")\n",
    "for sheet_name in selected_sheets:\n",
    "    print(sheet_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7d7aabed-e767-4c73-a610-3ddd10524982",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sampa DataFrame shape: (258, 30)\n",
      "Nkrankwanta DataFrame shape: (258, 30)\n",
      "Drobo DataFrame shape: (258, 30)\n",
      "Wenchi DataFrame shape: (257, 25)\n",
      "Techiman DataFrame shape: (257, 29)\n",
      "Sawla DataFrame shape: (257, 28)\n"
     ]
    }
   ],
   "source": [
    "# Convert the drying sheets into separate DataFrames\n",
    "sampa_df = pd.read_excel(xls_dispatches, sheet_name='SAMPA-DISPATCHES ', header=2)\n",
    "nkrankwanta_df = pd.read_excel(xls_dispatches, sheet_name='NKRANKWANTA-DISPATCHES', header=2)\n",
    "drobo_df = pd.read_excel(xls_dispatches, sheet_name='DROBO-DISPATCHES', header=2)\n",
    "wenchi_df = pd.read_excel(xls_dispatches, sheet_name='WENCHI-DISPATCHES', header=2)\n",
    "techiman_df = pd.read_excel(xls_dispatches, sheet_name='TECHIMAN-DISPATCHES', header=2)\n",
    "sawla_df = pd.read_excel(xls_dispatches, sheet_name='SAWLA-DISPATCHES', header=2)\n",
    "\n",
    "# Display the shape of each DataFrame for verification\n",
    "print(f\"Sampa DataFrame shape: {sampa_df.shape}\")\n",
    "print(f\"Nkrankwanta DataFrame shape: {nkrankwanta_df.shape}\")\n",
    "print(f\"Drobo DataFrame shape: {drobo_df.shape}\")\n",
    "print(f\"Wenchi DataFrame shape: {wenchi_df.shape}\")\n",
    "print(f\"Techiman DataFrame shape: {techiman_df.shape}\")\n",
    "print(f\"Sawla DataFrame shape: {sawla_df.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "adaff590-4e88-417b-8b9a-ac48d8464b4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Adding the new column 'Trc/Organic' to all DataFrames\n",
    "sampa_df['Trc/Organic'] = None  # Replace None with the appropriate default value or logic\n",
    "nkrankwanta_df['Trc/Organic'] = None\n",
    "drobo_df['Trc/Organic'] = None\n",
    "sawla_df['Trc/Organic'] = None\n",
    "wenchi_df.rename(columns={'TRC/ORGANIC': 'Trc/Organic'}, inplace=True)\n",
    "sawla_df.rename(columns={'Status': 'STATUS'}, inplace=True)\n",
    "# Confirm the new column has been added\n",
    "#print(sampa_df.head())\n",
    "#print(nkrankwanta_df.head())\n",
    "#print(drobo_df.head())\n",
    "#print(sawla_df.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "caea276a-7082-4375-a6be-1c2b4c9b7da8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Date  Date.1  Date.2\n",
      "0   NaN     NaN     0.0\n",
      "        Date     Date.1  Date.2\n",
      "0 2025-01-31 2025-02-03     0.0\n",
      "        Date  Date.1  Date.2\n",
      "0 2025-02-03     NaN     0.0\n",
      "        Date     Date.1  Date.2\n",
      "0 2025-01-22 2025-01-27     0.0\n",
      "        Date     Date.1  Date.2\n",
      "0 2025-01-22 2025-01-27     NaN\n",
      "   Date  Date.1  Date.2\n",
      "0   NaN     NaN     0.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\abayugo01\\AppData\\Local\\Temp\\ipykernel_19736\\4181643684.py:3: UserWarning: Parsing dates in %d-%m-%Y format when dayfirst=False (the default) was specified. Pass `dayfirst=True` or specify a format to silence this warning.\n",
      "  invalid_date_rows = df[df['Date'].apply(pd.to_datetime, errors='coerce').isna()]\n",
      "C:\\Users\\abayugo01\\AppData\\Local\\Temp\\ipykernel_19736\\4181643684.py:6: UserWarning: Parsing dates in %d-%m-%Y format when dayfirst=False (the default) was specified. Pass `dayfirst=True` or specify a format to silence this warning.\n",
      "  df['Date'] = df['Date'].apply(lambda x: pd.to_datetime(x, errors='coerce') if isinstance(x, str) else x)\n"
     ]
    }
   ],
   "source": [
    "# Check for rows where DATE or DATE.1 might have invalid date strings\n",
    "for df in [sampa_df, nkrankwanta_df, drobo_df, wenchi_df, techiman_df, sawla_df]:\n",
    "    invalid_date_rows = df[df['Date'].apply(pd.to_datetime, errors='coerce').isna()]\n",
    "    #print(invalid_date_rows[['DATE', 'DATE.1']])\n",
    "    # Ensure all dates are in the expected format (e.g., 'YYYY-MM-DD')\n",
    "    df['Date'] = df['Date'].apply(lambda x: pd.to_datetime(x, errors='coerce') if isinstance(x, str) else x)\n",
    "    df['Date.1'] = df['Date.1'].apply(lambda x: pd.to_datetime(x, errors='coerce') if isinstance(x, str) else x)\n",
    "    df['Date.2'] = df['Date.2'].apply(lambda x: pd.to_datetime(x, errors='coerce') if isinstance(x, str) else x)\n",
    "    print(df[['Date', 'Date.1', 'Date.2']].head(1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3f2c6a0f-b949-4123-895d-3196ac3e1217",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rows after removing NA based on 'Date':\n",
      "Sampa: 0\n",
      "Nkrankwanta: 1\n",
      "Drobo: 1\n",
      "Sawla: 0\n",
      "Techiman: 4\n",
      "Wenchi: 3\n"
     ]
    }
   ],
   "source": [
    "# Removing rows with NA in the 'Date' column for all DataFrames\n",
    "sampa_df = sampa_df.dropna(subset=['Date'])\n",
    "nkrankwanta_df = nkrankwanta_df.dropna(subset=['Date'])\n",
    "drobo_df = drobo_df.dropna(subset=['Date'])\n",
    "sawla_df = sawla_df.dropna(subset=['Date'])\n",
    "techiman_df = techiman_df.dropna(subset=['Date'])\n",
    "wenchi_df = wenchi_df.dropna(subset=['Date'])\n",
    "\n",
    "# Display the updated DataFrames to verify the changes\n",
    "print(\"Rows after removing NA based on 'Date':\")\n",
    "print(\"Sampa:\", len(sampa_df))\n",
    "print(\"Nkrankwanta:\", len(nkrankwanta_df))\n",
    "print(\"Drobo:\", len(drobo_df))\n",
    "print(\"Sawla:\", len(sawla_df))\n",
    "print(\"Techiman:\", len(techiman_df))\n",
    "\n",
    "print(\"Wenchi:\", len(wenchi_df) if 'wenchi_df' in locals() else \"Not included\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b2526cd3-1ba6-46e5-bf6f-3756e00cda35",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add Station column for each DataFrame\n",
    "sampa_df['Area'] = 'Sampa'\n",
    "nkrankwanta_df['Area'] = 'Nkrankwanta'\n",
    "drobo_df['Area'] = 'Drobo'\n",
    "wenchi_df['Area'] = 'Wenchi'\n",
    "techiman_df['Area'] = 'Techiman'\n",
    "sawla_df['Area'] = 'Sawla'\n",
    "\n",
    "# Get the column names from each DataFrame\n",
    "sampa_columns = set(sampa_df.columns)\n",
    "nkrankwanta_columns = set(nkrankwanta_df.columns)\n",
    "drobo_columns = set(drobo_df.columns)\n",
    "wenchi_columns = set(wenchi_df.columns)\n",
    "techiman_columns = set(techiman_df.columns)\n",
    "sawla_columns = set(sawla_df.columns)\n",
    "\n",
    "# Find the intersection (common columns) across all the DataFrames\n",
    "common_columns = sampa_columns.intersection(nkrankwanta_columns, drobo_columns, wenchi_columns, techiman_columns, sawla_columns)\n",
    "\n",
    "# Convert common_columns to a list\n",
    "common_columns_list = list(common_columns)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "408fa1ce-0e01-4e93-ad5b-29a82ceb4a6f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Combined DataFramer:\n",
      "(9, 26)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\abayugo01\\AppData\\Local\\Temp\\ipykernel_19736\\1319441565.py:15: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
      "  combined_df = pd.concat(dfs_ordered, ignore_index=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dispatches DataFrame has been saved to Ghana RCN Dispatches.xlsx\n"
     ]
    }
   ],
   "source": [
    "# Desired column order\n",
    "column_order = [\n",
    "    'STATUS', 'Area', 'Date', 'Waybill #', 'Truck #', '# of bags', 'Net weight', \n",
    "    'Nut Count', 'Moisture', 'Kor', 'Date.1', '# of bags.1', 'Net weight.1', \n",
    "    'Weight loss/gain.1', '% loss/gain.1', 'Nut count.1', 'Moisture.1', \n",
    "    'Kor.1', 'Date.2', '# of bags.2', 'Net weight.2', 'Weight loss/gain.2', \n",
    "    'NC.2', 'Moisture.2', 'Kor.2', 'Trc/Organic'\n",
    "]\n",
    "\n",
    "# Ensure all DataFrames have the same columns and order them\n",
    "dfs = [techiman_df, sawla_df, sampa_df, wenchi_df, drobo_df, nkrankwanta_df]\n",
    "dfs_ordered = [df.reindex(columns=column_order) for df in dfs]\n",
    "\n",
    "# Concatenate all DataFrames\n",
    "combined_df = pd.concat(dfs_ordered, ignore_index=True)\n",
    "\n",
    "# Sort the DataFrame by the 'Date' column in ascending order (oldest to newest)\n",
    "combined_df = combined_df.sort_values(by='Date', ascending=True)\n",
    "\n",
    "# Display the resulting DataFrame\n",
    "print(\"Combined DataFramer:\")\n",
    "print(combined_df.shape)\n",
    "combined_df.head(3)\n",
    "\n",
    "# Save the combined DataFrame to an Excel file\n",
    "output_file = 'Ghana RCN Dispatches.xlsx'\n",
    "\n",
    "# Save to Excel, specifying the sheet name and index preference\n",
    "combined_df.to_excel(output_file, index=False, sheet_name='Combined Dispatches')\n",
    "\n",
    "print(f\"Dispatches DataFrame has been saved to {output_file}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d52634cb-95d8-46a0-b6a0-b3050e627c9a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'# of bags',\n",
       " '# of bags.1',\n",
       " '# of bags.2',\n",
       " '% loss/gain',\n",
       " 'Area',\n",
       " 'Date',\n",
       " 'Date.1',\n",
       " 'Date.2',\n",
       " 'Kor',\n",
       " 'Kor.1',\n",
       " 'Kor.2',\n",
       " 'Moisture',\n",
       " 'Moisture.1',\n",
       " 'Moisture.2',\n",
       " 'NC',\n",
       " 'Net weight',\n",
       " 'Net weight.1',\n",
       " 'Net weight.2',\n",
       " 'Nut Count',\n",
       " 'Nut count',\n",
       " 'STATUS',\n",
       " 'Trc/Organic',\n",
       " 'Truck #',\n",
       " 'Waybill #',\n",
       " 'Weight loss/gain',\n",
       " 'Weight loss/gain.1'}"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "common_columns"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
